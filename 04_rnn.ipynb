{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d623ff5-86ee-43d4-942b-2de4c63b7118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from math import floor\n",
    "import math\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eade3051",
   "metadata": {},
   "source": [
    "## 单向 RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c53a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(input, weight_x, weight_h, bias_x, bias_h, prev_h=None):\n",
    "    \"\"\"\n",
    "    param:\n",
    "        input: [batch_size, seq_len, input_size]\n",
    "        weight_x: [input_size, hidden_size]\n",
    "        weight_h: [hidden_size, hidden_size]\n",
    "        prev_h: [batch_size, hidden_size]\n",
    "    \"\"\"    \n",
    "    batch_size, seq_len, input_size = input.shape\n",
    "    input_size, hidden_size = weight_x.shape\n",
    "\n",
    "    out_put = np.zeros((batch_size, seq_len, hidden_size))\n",
    "\n",
    "    if prev_h is None:\n",
    "        prev_h = np.zeros((batch_size, hidden_size))\n",
    "    \n",
    "    if bias_x is None:\n",
    "        bias_x = np.zeros((hidden_size))\n",
    "    if bias_h is None:\n",
    "        bias_h = np.zeros((hidden_size))\n",
    "\n",
    "    for i in range(seq_len):\n",
    "        cur_x = input[:, i, :] # 当前时刻的输入\n",
    "        cur_h = np.tanh(cur_x @ weight_x + prev_h @ weight_h + bias_x + bias_h)\n",
    "        out_put[:,i, :] = cur_h\n",
    "        prev_h = cur_h\n",
    "\n",
    "    \n",
    "    return out_put, prev_h.reshape(1, batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "665a3c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 输出 torch.Size([8, 7, 4]) torch.Size([1, 8, 4])\n",
      "手写rnn 输出 (8, 7, 4) (1, 8, 4)\n",
      "True True\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "batch_size, seq_len, input_size = 8, 7, 3\n",
    "hidden_size = 4\n",
    "\n",
    "x = np.random.randn(batch_size, seq_len, input_size,)\n",
    "x_ = torch.from_numpy(x).to(torch.float32)\n",
    "rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "torch_output, torch_hn = rnn(x_)\n",
    "\n",
    "\n",
    "# 提取 rnn 中的参数出来使用\n",
    "params = list(rnn.named_parameters())\n",
    "w_x = params[0][1].detach().numpy().T\n",
    "w_h = params[1][1].detach().numpy().T\n",
    "b_x = params[2][1].detach().numpy()\n",
    "b_h = params[3][1].detach().numpy()\n",
    "\n",
    "my_output, my_hn = rnn_forward(x, w_x, w_h, b_x, b_h)\n",
    "\n",
    "print(\"torch 输出\", torch_output.shape, torch_hn.shape)\n",
    "print(\"手写rnn 输出\", my_output.shape, my_hn.shape)\n",
    "print(np.allclose(my_output, torch_output.detach().numpy()), np.allclose(my_hn, torch_hn.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ade699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnn(X):\n",
    "    W = np.random.normal(0, 1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf3bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class RNNCell:\n",
    "    def __init__(self, input_size, hidden_size, bias, nonlinearity='tanh'):\n",
    "        \"\"\"\n",
    "        parameters:\n",
    "            input_size: The number of expected features in the input x\n",
    "            hidden_size: The number of features in the hidden state h\n",
    "            bias: If False, then the layer does not use bias weights b. Default: True\n",
    "            nonlinearity: The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'\n",
    "        \"\"\"\n",
    "        # init_param \n",
    "        self.W_hh = np.random.normal(loc=0, scale=np.sqrt(1/hidden_size))\n",
    "        self.W_hx = np.random.normal(loc=0, scale=np.sqrt(2/(input_size+hidden_size)))\n",
    "        self.bias = bias\n",
    "        if self.bias:\n",
    "            self.b = np.zeros((hidden_size))\n",
    "        self.nonlinearity = nonlinearity\n",
    "\n",
    "\n",
    "    def forward(self, x, prev_h):\n",
    "        \"\"\"input_ :[batch_size, input_size]\n",
    "        \"\"\"\n",
    "        a = prev_h @ self.W_hh + x @ self.W_hx\n",
    "        if self.bias:\n",
    "            a += self.b\n",
    "        if self.nonlinearity == \"tanh\":\n",
    "            a = np.tanh(a)\n",
    "        elif self.nonlinearity == \"relu\":\n",
    "            a = self.relu(a)\n",
    "        return a\n",
    "\n",
    "\n",
    "    def __call__(self, *arg):\n",
    "        return self.forward(*arg)     \n",
    "\n",
    "    def backward(self, pre_grad):\n",
    "        pass\n",
    "\n",
    "    def relu(self, x):\n",
    "        mask = x < 0\n",
    "        x[mask] = 0\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12664531",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, input_size, hidden_size, bias, nonlinearity):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.cell = RNNCell(self.input_size, self.hidden_size, bias, nonlinearity)\n",
    "        \n",
    "    def forward(self, input_, prev_h):\n",
    "        \"\"\"\n",
    "        parameters:\n",
    "            input_ :[batch_size, seq_len, input_size]\n",
    "            prev_h: [batch_size, seq_len, hidden_size]\n",
    "        return :\n",
    "            outputs: [batch_size, seq_len, hidden_size]\n",
    "            cur_h: [1, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, input_size = input_.shape\n",
    "\n",
    "        outputs = np.zeros((batch_size, seq_len, self.hidden_size))\n",
    "\n",
    "        for i in range(0, seq_len):\n",
    "            cur_x = input_[:, i, :]\n",
    "            cur_h = self.cell(cur_x, prev_h)\n",
    "            \n",
    "            outputs[:, i, :] = cur_h\n",
    "            prev_h = cur_h\n",
    "\n",
    "        return outputs, cur_h.reshape(1, batch_size, self.hidden_size)  \n",
    "\n",
    "    def __call__(self, *arg):\n",
    "        return self.forward(*arg)\n",
    "\n",
    "    def backward(self, pre_grad):\n",
    "        pass      "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07fac03186cb4872619664373be12749392d231b19ef18aa05ccb0afd9b23cee"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
